# Clean Slate Validation Plan

**Date**: December 2, 2025  
**Goal**: Systematic stage-by-stage validation with batch processing and space-saving cleanup

---

## Critical Principles

### 1. Only Test Production Code
- Run actual `generate_modern_json` executable
- Test actual algorithm functions, not test-specific code
- Use real PDB files as input
- Compare actual JSON output

### 2. Stage-by-Stage with Selective JSON Generation
- Modify `generate_modern_json` to output ONLY the stage being tested
- Don't generate all 10 JSON types at once
- Saves space and makes debugging clearer

### 3. Batch Processing with Cleanup
- Test in batches (e.g., 100 PDBs at a time)
- Keep modern JSON only if there's a mismatch
- Delete modern JSON after successful validation
- Keep legacy JSON always (it's the reference)

### 4. Incremental Validation
- Fix issues before moving to next stage
- Don't proceed if current stage has failures
- Build confidence stage-by-stage

---

## What to Keep vs Delete

### ‚úÖ ALWAYS KEEP (Never Delete)
```
data/pdb/                  # Input PDB files
resources/                 # Templates and configs
```

### üóëÔ∏è DELETE (Can Regenerate)
```
data/json/                 # Modern JSON - delete, regenerate stage-by-stage
data/json_legacy/          # Legacy JSON - delete, regenerate ONLY what we need per stage
data/validation_results/   # Old validation results - delete, regenerate
```

### Strategy: Generate Only What We Need
- **Legacy JSON**: Generate only for current stage, only for current batch
- **Modern JSON**: Generate only for current stage, only for current batch
- **After batch passes**: Delete both legacy AND modern JSON for that batch
- **After batch fails**: Keep modern/legacy JSON for failed PDBs (debugging)
- **Result**: Minimal disk usage - only keep failures

---

## Stage Definitions with Production Code Mapping

### Stage 1: Atoms
**What**: Parse PDB atoms correctly

**Production Code Being Tested**:
- `src/x3dna/io/pdb_parser.cpp::parse()`
- `src/x3dna/models/atom.cpp`

**JSON Type**: `pdb_atoms`

**Generated By**: 
```cpp
// In generate_modern_json.cpp or tools
writer.write_pdb_atoms(structure);
```

**Success**: Every atom matches legacy (coordinates, names, etc.)

---

### Stage 2: Reference Frames  
**What**: Calculate reference frames for each base

**Production Code Being Tested**:
- `src/x3dna/algorithms/base_frame_calculator.cpp::calculate_frame()`
- `src/x3dna/algorithms/base_frame_calculator.cpp::ls_fitting()`

**JSON Types**: `base_frame_calc`, `frame_calc`

**Generated By**:
```cpp
// In FindPairProtocol
BaseFrameCalculator calculator;
calculator.calculate_all_frames(structure);

// JSON recording
writer.record_base_frame_calc(...);
writer.record_frame_calc(...);
```

**Success**: All frames match legacy (rotation matrices, origins)

---

### Stage 3: Distance Checks
**What**: Calculate geometric measurements between base pairs

**Production Code Being Tested**:
- `src/x3dna/algorithms/base_pair_validator.cpp::validate()`
- Specifically the distance calculation parts (dorg, dNN, plane_angle, d_v)

**JSON Type**: `distance_checks`

**Generated By**:
```cpp
// In BasePairFinder::find_pairs_with_recording()
auto validation = validator.validate(res_i, res_j);
writer.record_distance_checks(i, j, validation);
```

**Success**: All geometric measurements match legacy

---

### Stage 4: H-Bond Detection
**What**: Detect hydrogen bonds between bases

**Production Code Being Tested**:
- `src/x3dna/algorithms/hydrogen_bond_finder.cpp::find_hbonds()`
- `src/x3dna/algorithms/hydrogen_bond_finder.cpp::resolve_conflicts()`

**JSON Type**: `hbond_list`

**Generated By**:
```cpp
// In BasePairValidator::validate()
auto hbonds = HydrogenBondFinder::find_hbonds(res_i, res_j);
// ... conflict resolution ...
writer.record_hbonds(i, j, final_hbonds);
```

**Success**: All H-bonds match legacy (donors, acceptors, distances)

---

### Stage 5: Pair Validation
**What**: Determine which pairs pass validation thresholds

**Production Code Being Tested**:
- `src/x3dna/algorithms/base_pair_validator.cpp::validate()`
- Complete validation logic with all checks

**JSON Type**: `pair_validation`

**Generated By**:
```cpp
// In BasePairFinder::find_pairs_with_recording()
auto validation = validator.validate(res_i, res_j);
writer.record_pair_validation(i, j, validation);
```

**Success**: All validation decisions match legacy (is_valid, quality scores)

---

### Stage 6: Pair Selection ‚≠ê PRIMARY OUTPUT
**What**: Select final base pairs using greedy algorithm

**Production Code Being Tested**:
- `src/x3dna/algorithms/base_pair_finder.cpp::find_pairs()`
- Greedy selection algorithm
- Mutual best pair matching

**JSON Types**: `find_bestpair_selection`, `base_pair`

**Generated By**:
```cpp
// In BasePairFinder::find_pairs()
// Final selection after greedy algorithm
writer.record_bestpair_selection(i, j, selected_pairs);
writer.record_base_pair(base_pair);
```

**Success**: Selected pairs match legacy exactly (MOST CRITICAL!)

---

### Stage 7: Step Parameters
**What**: Calculate step parameters for consecutive pairs

**Production Code Being Tested**:
- `src/x3dna/algorithms/parameter_calculator.cpp::calculate_step_parameters()`
- `src/x3dna/algorithms/parameter_calculator.cpp::bpstep_par_impl()`

**JSON Type**: `bpstep_params`

**Generated By**:
```cpp
// In AnalyzeProtocol::execute()
ParameterCalculator calc;
auto params = calc.calculate_step_parameters(base_pairs, structure);
writer.record_bpstep_params(params);
```

**Success**: All 6 parameters match legacy (Shift, Slide, Rise, Tilt, Roll, Twist)

---

### Stage 8: Helical Parameters
**What**: Calculate helical parameters

**Production Code Being Tested**:
- `src/x3dna/algorithms/parameter_calculator.cpp::calculate_helical_parameters()`

**JSON Type**: `helical_params`

**Generated By**:
```cpp
// In AnalyzeProtocol::execute()
auto helical = calc.calculate_helical_parameters(base_pairs, structure);
writer.record_helical_params(helical);
```

**Success**: All helical parameters match legacy

---

## Implementation Plan

### Step 1: Clean Slate - Delete Everything ‚úÖ DO FIRST

```bash
# Delete modern JSON (can regenerate)
rm -rf data/json/

# Delete legacy JSON (can regenerate - saves HUGE space)
rm -rf data/json_legacy/

# Delete old validation results
rm -rf data/validation_results/

# Create fresh directories
mkdir -p data/json
mkdir -p data/json_legacy
mkdir -p data/validation_results

echo "‚úÖ Clean slate - ready to generate on-demand!"
```

**Philosophy**: 
- Don't pre-generate or keep everything
- Generate ONLY what we need, WHEN we need it, for current batch/stage
- Delete after successful validation (even legacy!)
- Keep only: PDB files (inputs) + result JSONs (outcomes) + failure cases

---

### Step 2: Modify generate_modern_json for Selective Output
Currently `generate_modern_json` outputs ALL JSON types. We need to add flags to output only specific stages:

```cpp
// Add command-line options:
--output-stage=atoms          // Only output pdb_atoms
--output-stage=frames         // Only output base_frame_calc, frame_calc
--output-stage=distances      // Only output distance_checks
--output-stage=hbonds         // Only output hbond_list
--output-stage=validation     // Only output pair_validation
--output-stage=selection      // Only output find_bestpair_selection, base_pair
--output-stage=all            // Default: output everything
```

**Alternative**: Create 8 separate executables (one per stage)
- Cleaner separation
- No flag management
- Each tests specific production code

---

### Step 3: Create Batch Validation Script

**New Script**: `scripts/batch_validation.py`

```python
# Pseudo-code:
for batch in batches_of_100(valid_pdbs):
    # 1. Generate LEGACY JSON for this batch (current stage only)
    generate_legacy_json_for_batch(batch, current_stage)
    
    # 2. Generate MODERN JSON for this batch (current stage only)
    generate_modern_json_for_batch(batch, current_stage)
    
    # 3. Validate
    results = validate_batch(batch, current_stage)
    
    # 4. Cleanup (AGGRESSIVE - delete everything that passed)
    for pdb in batch:
        if results[pdb].match:
            # Perfect match - DELETE BOTH legacy and modern
            delete_legacy_json(pdb, current_stage)
            delete_modern_json(pdb, current_stage)
            # Record result only
        else:
            # Mismatch - KEEP BOTH for debugging
            keep_legacy_json(pdb, current_stage)
            keep_modern_json(pdb, current_stage)
            record_issue(pdb, results[pdb].issue)
    
    # 5. Update stage result file (this is what we keep!)
    update_stage_results(current_stage, results)
```

**Space Savings**:
- Only keep JSON for failures
- If 95% pass: Keep only ~200 PDBs √ó stage's JSON types
- Massive space savings!

---

### Step 4: Validation Workflow

#### Stage 2: Frames (Start Here - Stage 1 atoms is less critical)

**Batch 1** (PDBs 1-100):
```bash
# Generate modern JSON (frames only)
for pdb in 100D 157D ... (first 100); do
  ./build/generate_modern_json --output-stage=frames \
    --fix-indices \
    data/pdb/${pdb}.pdb \
    data/json/
done

# Validate
python3 scripts/stage_by_stage_validation.py \
  --stage stage2_frames \
  --batch 0 \
  --batch-size 100

# Check results
# If all pass: delete modern JSON for batch
# If failures: keep failed PDBs, investigate

# Record results
# Update stage2_frames_results.json with batch 1 results
```

**Batch 2** (PDBs 101-200):
- Repeat process
- Only keep modern JSON for failures

**Continue** until all 4,123 PDBs tested for Stage 2

**Goal**: Stage 2 at 100% pass rate before moving to Stage 3

---

#### Stage 3: Distance Checks (After Stage 2 Complete)

Same process:
- Generate distance_checks JSON only
- Validate batch by batch
- Delete successful, keep failures
- Fix failures before moving to Stage 4

---

#### Stages 4-8: Repeat

Each stage:
1. Generate only that stage's JSON
2. Validate in batches
3. Cleanup successes
4. Fix failures
5. Achieve 100% before next stage

---

## Modified generate_modern_json Design

### Option A: Add Flags (Simplest)

```cpp
// In generate_modern_json.cpp
struct OutputOptions {
    bool output_pdb_atoms = true;
    bool output_frames = true;
    bool output_distances = true;
    bool output_hbonds = true;
    bool output_validation = true;
    bool output_selection = true;
    // Note: steps/helical need analyze, not find_pair
};

// Parse --output-stage flag
if (stage == "frames") {
    options.output_pdb_atoms = false;
    options.output_frames = true;
    options.output_distances = false;
    // ... etc
}

// Use throughout JSON writing
if (options.output_pdb_atoms) {
    writer.write_pdb_atoms(structure);
}
```

### Option B: Separate Executables (Cleanest)

Create:
- `validate_stage1_atoms` - Only outputs pdb_atoms
- `validate_stage2_frames` - Only outputs base_frame_calc, frame_calc
- `validate_stage3_distances` - Only outputs distance_checks
- etc.

Each executable:
1. Runs the real production code
2. Outputs only its stage's JSON
3. Clear what's being tested

---

## Batch Validation Script

**New Script**: `scripts/batch_validation_workflow.py`

```python
#!/usr/bin/env python3
"""
Batch validation with automatic cleanup.

Process:
1. Generate modern JSON for batch (current stage only)
2. Validate against legacy
3. Delete modern JSON if match (save space)
4. Keep modern JSON if mismatch (debug)
5. Update results JSON
"""

def validate_batch(stage_id, batch_start, batch_size):
    """Validate one batch of PDBs for a specific stage."""
    # 1. Get batch of PDBs
    pdbs = valid_pdbs[batch_start:batch_start+batch_size]
    
    # 2. Generate modern JSON (ONLY for this stage)
    for pdb in pdbs:
        generate_stage_json(pdb, stage_id)
    
    # 3. Validate
    results = validate_stage_for_pdbs(pdbs, stage_id)
    
    # 4. Cleanup
    for pdb, result in results.items():
        if result['match']:
            # Perfect match - delete to save space
            delete_modern_json(pdb, stage_id)
            print(f"  ‚úÖ {pdb}: PASS - cleaned up")
        else:
            # Mismatch - keep for investigation
            print(f"  ‚ùå {pdb}: FAIL - kept for debugging")
            print(f"      Issue: {result['issue']}")
    
    # 5. Update results file
    update_results_file(stage_id, results)
    
    return results

def generate_stage_json(pdb_id, stage_id):
    """Generate modern JSON for only the specified stage."""
    stage_info = STAGES[stage_id]
    
    # Run appropriate tool with stage-specific flags
    if stage_id in ['stage1_atoms', 'stage2_frames', 'stage3_distances', 
                    'stage4_hbonds', 'stage5_validation', 'stage6_selection']:
        # Use find_pair-based generation
        cmd = [
            './build/generate_modern_json',
            f'--output-stage={stage_info["output_flag"]}',
            '--fix-indices',
            f'data/pdb/{pdb_id}.pdb',
            'data/json/'
        ]
    else:
        # Stages 7-8 need analyze
        cmd = [
            './build/find_pair_app',
            '--fix-indices',
            f'data/pdb/{pdb_id}.pdb',
            f'/tmp/{pdb_id}.inp'
        ]
        # Then run analyze with stage flag
        cmd2 = [
            './build/analyze_app',
            f'--output-stage={stage_info["output_flag"]}',
            f'/tmp/{pdb_id}.inp'
        ]
    
    # Execute
    subprocess.run(cmd, ...)
```

---

## Execution Plan

### Phase 1: Setup ‚úÖ DO FIRST

```bash
# 1. Archive existing data
mkdir -p data/archive/clean_slate_$(date +%Y%m%d_%H%M%S)
mv data/json data/archive/clean_slate_$(date +%Y%m%d_%H%M%S)/json_modern
mv data/validation_results data/archive/clean_slate_$(date +%Y%m%d_%H%M%S)/validation_results

# 2. Create fresh directories
mkdir -p data/json
mkdir -p data/validation_results

# 3. Verify legacy JSON intact
ls data/json_legacy/ | wc -l  # Should show directories

echo "‚úÖ Clean slate ready!"
```

---

### Phase 2: Modify Code for Selective Output

#### Option A: Quick Approach - Modify generate_modern_json

Add `--output-types` flag:
```cpp
// Parse comma-separated list
// --output-types=pdb_atoms,base_frame_calc,frame_calc

std::set<std::string> enabled_types;
// Parse from args

// Then in code:
if (enabled_types.count("pdb_atoms") || enabled_types.empty()) {
    writer.write_pdb_atoms(structure);
}
```

#### Option B: Cleaner Approach - Stage-Specific Tools

Create wrapper scripts:
```bash
# scripts/generate_stage1.sh
./build/generate_modern_json data/pdb/$1.pdb data/json/ --output-types=pdb_atoms

# scripts/generate_stage2.sh  
./build/generate_modern_json data/pdb/$1.pdb data/json/ --output-types=base_frame_calc,frame_calc

# etc.
```

---

### Phase 3: Batch Validation Loop

#### For Stage 2 (Frames - Start Here)

```bash
# Batch 1: First 100 PDBs
python3 scripts/batch_validation_workflow.py \
  --stage stage2_frames \
  --batch-start 0 \
  --batch-size 100 \
  --cleanup

# Check results
cat data/validation_results/stage2_frames_results.json | jq '.summary'

# If failures:
cat data/validation_results/stage2_frames_results.json | jq '.failed[0:5]'

# Investigate first failure
python3 scripts/investigate_failure.py stage2_frames <PDB_ID>

# Fix issue in code
# Rebuild
make release

# Re-run batch 1
python3 scripts/batch_validation_workflow.py \
  --stage stage2_frames \
  --batch-start 0 \
  --batch-size 100 \
  --cleanup \
  --revalidate

# Once batch 1 is 100%: move to batch 2
# Repeat for all ~42 batches
```

---

### Phase 4: Complete All Stages

**Stage 2: Frames**
- 42 batches √ó 100 PDBs
- Fix failures as they appear
- Achieve 100% pass rate
- ‚úÖ Stage 2 complete

**Stage 3: Distance Checks**  
- Same process
- Only generate distance_checks JSON
- Clean up after successful batches
- ‚úÖ Stage 3 complete

**Stages 4-8**: Repeat

---

## Aggressive Space-Saving Strategy

### Without Cleanup (Old Approach)
If we keep all legacy + modern JSON for all stages:
- 4,123 PDBs √ó 10 types √ó 2 (legacy + modern) √ó ~20KB = **~1.6 GB**

### With Aggressive Cleanup (New Approach)
Only keep failures (delete BOTH legacy AND modern after successful validation):
- If 5% fail: 206 PDBs √ó 10 types √ó 2 √ó ~20KB = **~82 MB**
- **Saves ~95% of space!**
- Result JSON files are tiny (~100KB each)

### What Gets Deleted
```bash
# After batch validates successfully:
# Delete BOTH legacy and modern JSON for successful PDBs
rm data/json_legacy/base_frame_calc/100D.json  # If 100D passed
rm data/json/base_frame_calc/100D.json         # If 100D passed

# What we KEEP:
# - PDB files (data/pdb/)
# - Result JSON files (data/validation_results/*.json)
# - JSON files for failures only (for debugging)
```

### On-Demand Generation
```bash
# For each batch:
# 1. Generate legacy JSON for batch (current stage only)
for pdb in batch; do
  cd org && ./build/bin/find_pair_analyze ../data/pdb/${pdb}.pdb
done

# 2. Generate modern JSON for batch (current stage only)
for pdb in batch; do
  ./build/generate_modern_json --output-types=<stage> data/pdb/${pdb}.pdb data/json/
done

# 3. Validate
# 4. Delete both if passed
# 5. Keep both if failed
```

---

## Validation Result Files Structure

Each stage result file tracks progress:

```json
{
  "stage_id": "stage2_frames",
  "stage_name": "Reference Frames",
  "total_pdbs": 4123,
  "batches_completed": 5,
  "passed": ["100D", "157D", ...],  // List grows
  "failed": [
    {
      "pdb_id": "7EH2",
      "issue": "Count mismatch: legacy=88, modern=48",
      "batch": 3,
      "details": {...}
    }
  ],
  "summary": {
    "passed_count": 450,
    "failed_count": 50,
    "untested_count": 3623,
    "pass_rate": 90.0
  }
}
```

**Incremental updates**: Each batch adds to these files

---

## Ensuring Production Code is Tested

### What We're Testing

**NOT testing**: Test-specific mock functions  
**YES testing**: Actual production executables

**How**:
1. Run `./build/generate_modern_json` (production executable)
2. It calls production code:
   - `PdbParser::parse()` ‚Üí generates pdb_atoms
   - `BaseFrameCalculator::calculate_frame()` ‚Üí generates frames
   - `BasePairFinder::find_pairs()` ‚Üí generates selections
   - etc.
3. Compare output JSON with legacy JSON
4. Any mismatch = production code bug

**Verification**:
- Read source code to confirm which functions generate which JSON
- Trace from executable ‚Üí protocol ‚Üí algorithm ‚Üí JSON writer
- Document the call chain in this file (see stage definitions above)

---

## Implementation Checklist

### Code Changes Needed

- [ ] Modify `generate_modern_json.cpp` to accept `--output-types` flag
- [ ] Add selective JSON output logic
- [ ] Test flag works correctly
- [ ] Rebuild

**OR**

- [ ] Create stage-specific wrapper scripts
- [ ] Each script calls generate_modern_json with appropriate filters

### Script Creation

- [ ] Create `scripts/batch_validation_workflow.py`
- [ ] Implement batch processing
- [ ] Implement cleanup logic
- [ ] Implement incremental result updates
- [ ] Test on small batch (10 PDBs)

### Validation Process

- [ ] Archive existing data
- [ ] Start with Stage 2 (frames)
- [ ] Batch 1: 100 PDBs
- [ ] Fix any failures
- [ ] Continue batches until stage complete
- [ ] Move to Stage 3
- [ ] Repeat for all 8 stages

---

## Expected Timeline

### Optimistic (If Few Failures)
- **Stage 2**: 2-3 hours (42 batches)
- **Stage 3**: 2-3 hours  
- **Stage 4**: 2-3 hours
- **Stage 5**: 2-3 hours
- **Stage 6**: 2-3 hours (CRITICAL - may need more debugging)
- **Stage 7**: 1-2 hours (only 23 legacy files)
- **Stage 8**: 1-2 hours (only 23 legacy files)

**Total**: ~15-20 hours of validation time

### Realistic (With Debugging)
- **Stage 2**: 1 day (fix frame calculation issues)
- **Stage 3**: 1 day
- **Stage 4**: 1-2 days (H-bonds are complex)
- **Stage 5**: 1 day
- **Stage 6**: 2-3 days (CRITICAL - greedy algorithm)
- **Stage 7**: 1 day
- **Stage 8**: 1 day

**Total**: ~1-2 weeks

---

## Success Criteria

### Per Stage
- ‚úÖ 100% pass rate (all PDBs with legacy JSON)
- ‚úÖ All production code functions tested
- ‚úÖ Result JSON file documents all outcomes
- ‚úÖ Failures investigated and fixed

### Overall
- ‚úÖ All 8 stages at 100%
- ‚úÖ All 4,123 valid PDBs pass
- ‚úÖ Production code verified correct
- ‚úÖ Documentation complete

---

## Notes

### Why Start from Scratch?
- Unclear what was tested in existing modern JSON
- Count mismatches suggest systematic issues
- Clean slate ensures we test systematically
- Archive preserves existing work

### Why Batch Processing?
- Can't keep 4,123 √ó 10 JSON files in memory/disk
- Find issues incrementally
- Fix issues before they compound
- Progress tracking

### Why Delete Successes?
- Save disk space
- Focus attention on failures
- Can always regenerate if needed
- Legacy JSON is the reference

---

## Documentation This Plan Creates

As we execute:

1. **This file** (`CLEAN_SLATE_VALIDATION_PLAN.md`) - The plan
2. **8 result JSONs** (`stage*_results.json`) - The outcomes
3. **Investigation notes** - For each failure found
4. **Final report** - Overall validation success

---

**Next Action**: Implement the plan, starting with archiving existing data.

